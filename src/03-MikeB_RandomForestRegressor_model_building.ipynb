{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b118a39",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d278eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import r2_score \n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa03c634",
   "metadata": {},
   "source": [
    "## 1. Asset loading\n",
    "\n",
    "### 1.1. Feature information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb1f169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/biometric_features.pkl', 'rb') as input_file:\n",
    "    biometric_features=pickle.load(input_file)\n",
    "\n",
    "with open('../data/input_features.pkl', 'rb') as input_file:\n",
    "    input_features=pickle.load(input_file)\n",
    "\n",
    "with open('../data/output_features.pkl', 'rb') as input_file:\n",
    "    output_features=pickle.load(input_file)\n",
    "\n",
    "with open('../data/categorical_features.pkl', 'rb') as input_file:\n",
    "    categorical_features=pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb8b1a",
   "metadata": {},
   "source": [
    "### 1.2. Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bc2a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/train.pkl', 'rb') as input_file:\n",
    "    train_df=pickle.load(input_file)\n",
    "\n",
    "with open('../data/processed/test.pkl', 'rb') as input_file:\n",
    "    test_df=pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d112fbbc",
   "metadata": {},
   "source": [
    "## 2. Model Building\n",
    "\n",
    "We need to build two models - one to predict time and the other to predict calories. We will again use a dictionary to keep things organized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae144d",
   "metadata": {},
   "source": [
    "### 2.1. Model dictionary definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419ff33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using HistGradientBoostingRegressor for both models, Im assigning RandomForestRegressor to calorie_model while keeping Gradient Boosting for time_model.\n",
    "\n",
    "models={\n",
    "    'calorie_model': RandomForestRegressor(n_estimators=100, random_state=42), # Using RF for calorie prediction\n",
    "    'time_model': HistGradientBoostingRegressor(early_stopping=True) # Keeping HGB for duration prediction\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461bb83",
   "metadata": {},
   "source": [
    "### 2.2. Naive model cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b311be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calorie_model Naïve Cross-Validation RMSE: 12.67\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Calories Burned'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m cross_val_results={}\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     X_train = \u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m     y_train = train_df[output_features[model_name]]\n\u001b[32m     12\u001b[39m     scores = cross_val_score(model, X_train, y_train, cv=\u001b[32m5\u001b[39m, scoring=\u001b[33m\"\u001b[39m\u001b[33mneg_root_mean_squared_error\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6249\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6252\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['Calories Burned'] not in index\""
     ]
    }
   ],
   "source": [
    "# Since Im comparing Random Forest Regression (calorie_model) and HistGradientBoostingRegressor (time_model), I'll use cross-validation to test their performance before hyperparameter tuning.\n",
    "# Ensures models perform well across different data subsets\n",
    "# Provides a baseline RMSE before tuning hyperparameters\n",
    "# Helps compare effectiveness of Random Forest vs. Gradient Boosting before optimization\n",
    "\n",
    "cross_val_results={}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    X_train = train_df[input_features[model_name]]\n",
    "    y_train = train_df[output_features[model_name]]\n",
    "\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring=\"neg_root_mean_squared_error\")\n",
    "\n",
    "    cross_val_results[model_name] = abs(scores.mean())\n",
    "\n",
    "    print(f\"{model_name} Naïve Cross-Validation RMSE: {cross_val_results[model_name]:.2f}\")\n",
    "\n",
    "    # Your code here... Remember: the features names for each model are the biometric\n",
    "    # features + the 'extra' feature from the input features dictionary we\n",
    "    # loaded above. The label name is also stored in the output features dictionary.\n",
    "    # The keys match across the models, input features and output features dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a978f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train calorie_model\n",
    "models[\"calorie_model\"].fit(train_df[input_features[\"calorie_model\"]], train_df[output_features[\"calorie_model\"]])\n",
    "\n",
    "# Train time_model\n",
    "models[\"time_model\"].fit(train_df[input_features[\"time_model\"]], train_df[output_features[\"time_model\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52023b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I can't directly extract feature_importances_, I can use permutation_importance from sklearn.inspection, which evaluates how much each feature affects model performance\n",
    "\n",
    "def get_boosted_feature_importance(model, X_test, y_test, feature_names):\n",
    "    \"\"\"Uses permutation importance for HistGradientBoostingRegressor.\"\"\"\n",
    "    perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": perm_importance.importances_mean\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "# Apply function for time_model\n",
    "time_feature_importance = get_boosted_feature_importance(\n",
    "    models[\"time_model\"], \n",
    "    test_df[input_features[\"time_model\"]], \n",
    "    test_df[output_features[\"time_model\"]], \n",
    "    input_features[\"time_model\"]\n",
    ")\n",
    "\n",
    "print(\"Time Model Feature Importance:\\n\", time_feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a681c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies which biometric features impact predictions most.\n",
    "# decide which inputs to keep vs. remove for streamlined training.\n",
    "\n",
    "def get_feature_importance(model, feature_names):\n",
    "    \"\"\"Retrieves and sorts feature importance scores.\"\"\"\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": model.feature_importances_\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    return importance_df\n",
    "\n",
    "# Checking importance for calorie_model (Random Forest)\n",
    "calorie_feature_importance = get_feature_importance(models[\"calorie_model\"], input_features[\"calorie_model\"])\n",
    "print(\"Calorie Model Feature Importance:\\n\", calorie_feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we previously used OrdinalEncoder, we need to validate if One-Hot Encoding (OHE) would be better for categorical features.\n",
    "# Ordinal works well for ordered categories (e.g., Workout Intensity), but One-Hot is better for independent categories (e.g., Workout Type).\n",
    "\n",
    "# Define mapping for Gender\n",
    "gender_mapping = {0.0: \"Male\", 1.0: \"Female\", 2.0: \"Other\"}\n",
    "train_df[\"Gender\"] = train_df[\"Gender\"].map(gender_mapping)\n",
    "test_df[\"Gender\"] = test_df[\"Gender\"].map(gender_mapping)\n",
    "\n",
    "# Define mapping for Workout Type\n",
    "workout_mapping = {\n",
    "    0.0: \"Running\", 1.0: \"Cycling\", 2.0: \"Yoga\", \n",
    "    3.0: \"Swimming\", 4.0: \"Strength Training\", 5.0: \"HIIT\"\n",
    "}\n",
    "train_df[\"Workout Type\"] = train_df[\"Workout Type\"].map(workout_mapping)\n",
    "test_df[\"Workout Type\"] = test_df[\"Workout Type\"].map(workout_mapping)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Use correct parameter\n",
    "encoded_array = encoder.fit_transform(train_df[[\"Gender\", \"Workout Type\"]])\n",
    "\n",
    "# Retrieve meaningful feature names\n",
    "encoded_feature_names = encoder.get_feature_names_out([\"Gender\", \"Workout Type\"])\n",
    "\n",
    "# Convert to DataFrame with proper column names\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoded_feature_names)\n",
    "\n",
    "# Merge back into dataset\n",
    "train_df = train_df.drop(columns=[\"Gender\", \"Workout Type\"]).join(encoded_df)\n",
    "test_df = test_df.drop(columns=[\"Gender\", \"Workout Type\"]).join(pd.DataFrame(encoder.transform(test_df[[\"Gender\", \"Workout Type\"]]), columns=encoded_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get updated categorical feature names\n",
    "categorical_features = [\"Gender_Female\", \"Gender_Male\", \"Gender_Other\",\n",
    "                        \"Workout Type_Cycling\", \"Workout Type_HIIT\", \"Workout Type_Running\",\n",
    "                        \"Workout Type_Strength Training\", \"Workout Type_Swimming\", \"Workout Type_Yoga\"]\n",
    "\n",
    "# Update calorie_model input features\n",
    "input_features[\"calorie_model\"] = [f for f in input_features[\"calorie_model\"] if f not in [\"Gender\", \"Workout Type\"]] + categorical_features\n",
    "\n",
    "# Update time_model input features\n",
    "input_features[\"time_model\"] = [f for f in input_features[\"time_model\"] if f not in [\"Gender\", \"Workout Type\"]] + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ebd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since models like Gradient Boosting perform better with scaled features, we should normalize values like age, weight, and resting heart rate.\n",
    "\n",
    "# Define numeric features to scale\n",
    "numeric_features = [\"Age\", \"Height (cm)\", \"Weight (kg)\", \"Sleep Hours\", \"Resting Heart Rate (bpm)\", \"Body Fat (%)\"]\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "train_df[numeric_features] = scaler.fit_transform(train_df[numeric_features])\n",
    "test_df[numeric_features] = scaler.transform(test_df[numeric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56857314",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775cd8f8",
   "metadata": {},
   "source": [
    "## 3. Model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c1507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "    \"calorie_model\": {\n",
    "        \"n_estimators\": randint(50, 500),\n",
    "        \"max_depth\": randint(3, 30),\n",
    "        \"min_samples_split\": randint(2, 20),\n",
    "        \"min_samples_leaf\": randint(1, 20),\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "    },\n",
    "    \"time_model\": {\n",
    "        'max_iter': randint(10, 10000),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'max_depth': randint(3, 15),\n",
    "        'l2_regularization': uniform(0.0, 0.5)\n",
    "    }\n",
    "    # etc.\n",
    "}\n",
    "optimized_hyperparameters={}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    search = RandomizedSearchCV(model, hyperparameters[model_name], n_iter=200, cv=5, scoring=\"neg_root_mean_squared_error\", random_state=42)\n",
    "    search.fit(train_df[input_features[model_name]], train_df[output_features[model_name]])\n",
    "\n",
    "    models[model_name] = search.best_estimator_\n",
    "    optimized_hyperparameters[model_name] = search.best_params_\n",
    "\n",
    "    print(f\"{model_name} Best Hyperparameters:\\n\", optimized_hyperparameters[model_name])\n",
    "\n",
    "    # Your code here... HistGradientBoostingRegressor is fast on this dataset, so use\n",
    "    # RandomizedSearchCV with a few hundred or thousand iterations. Make sure to replace\n",
    "    # the naive model with the optimized one in the models dictionary and store the\n",
    "    # winning hyperparameters using the model name as key in the optimized hyperparameters\n",
    "    # dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4613ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_df, model_name):\n",
    "    \"\"\"Evaluates RMSE for a given model.\"\"\"\n",
    "    X_test = test_df[input_features[model_name]]\n",
    "    y_test = test_df[output_features[model_name]]\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False) if hasattr(mean_squared_error, 'squared') else mean_squared_error(y_test, y_pred) ** 0.5\n",
    "  # Root Mean Squared Error\n",
    "    \n",
    "    print(f\"{model_name} RMSE Before Tuning: {rmse:.2f}\")\n",
    "    return rmse\n",
    "\n",
    "# Evaluate pre-tuned models\n",
    "pre_tuned_rmse = {}\n",
    "for model_name, model in models.items():\n",
    "    pre_tuned_rmse[model_name] = evaluate_model(model, test_df, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d0a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain models with optimized hyperparameters\n",
    "for model_name, model in models.items():\n",
    "    model.fit(train_df[input_features[model_name]], train_df[output_features[model_name]])\n",
    "\n",
    "# Evaluate post-tuned models\n",
    "post_tuned_rmse = {}\n",
    "for model_name, model in models.items():\n",
    "    post_tuned_rmse[model_name] = evaluate_model(model, test_df, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f39adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print RMSE comparison\n",
    "for model_name in models.keys():\n",
    "    print(f\"{model_name} Improvement: {pre_tuned_rmse[model_name]:.2f} → {post_tuned_rmse[model_name]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b03025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for plotting\n",
    "models_list = list(models.keys())  # [\"calorie_model\", \"time_model\"]\n",
    "rmse_before = [pre_tuned_rmse[model] for model in models_list]\n",
    "rmse_after = [post_tuned_rmse[model] for model in models_list]\n",
    "\n",
    "# Create bar chart comparison\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(models_list, rmse_before, color='red', alpha=0.6, label=\"RMSE Before Tuning\")\n",
    "ax.bar(models_list, rmse_after, color='green', alpha=0.6, label=\"RMSE After Tuning\")\n",
    "\n",
    "# Labels & formatting\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.set_title(\"Model Performance Before vs. After Hyperparameter Tuning\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df714f80",
   "metadata": {},
   "source": [
    "## 4. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12367b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here... Make predictions on the test set with the optimized time and calorie models.\n",
    "# Then evaluate those predictions - plotting predicted vs true values and/or fit residuals is\n",
    "# a good idea, you also probably want to look at the RMSE between predictions and labels \n",
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluates RMSE and prints results.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False) if hasattr(mean_squared_error, 'squared') else mean_squared_error(y_test, y_pred) ** 0.5\n",
    "    print(f\"{model_name} RMSE: {rmse:.2f}\")\n",
    "    return rmse\n",
    "\n",
    "# Apply evaluation for both models\n",
    "post_tuned_rmse = {}\n",
    "for model_name, model in models.items():\n",
    "    X_test = test_df[input_features[model_name]]\n",
    "    y_test = test_df[output_features[model_name]]\n",
    "    post_tuned_rmse[model_name] = evaluate_model(model, X_test, y_test, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a2b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_r2(model, X_test, y_test, model_name):\n",
    "    \"\"\"Calculates R² score to assess fit.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{model_name} R² Score: {r2:.2f}\")\n",
    "    return r2\n",
    "\n",
    "# Apply evaluation for both models\n",
    "post_tuned_r2 = {}\n",
    "for model_name, model in models.items():\n",
    "    X_test = test_df[input_features[model_name]]\n",
    "    y_test = test_df[output_features[model_name]]\n",
    "    post_tuned_r2[model_name] = evaluate_r2(model, X_test, y_test, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f19563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(model, X_test, y_test, model_name):\n",
    "    \"\"\"Plots residuals to visualize errors.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.6)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel(\"Predicted Values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"{model_name} Residual Plot\")\n",
    "    plt.show()\n",
    "\n",
    "# Apply residual analysis for both models\n",
    "for model_name, model in models.items():\n",
    "    X_test = test_df[input_features[model_name]]\n",
    "    y_test = test_df[output_features[model_name]]\n",
    "    plot_residuals(model, X_test, y_test, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8f4273",
   "metadata": {},
   "source": [
    "## 5. Save assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e607da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('../models').mkdir(exist_ok=True)\n",
    "\n",
    "with open('../models/optimized_models.pkl', 'wb') as output_file:\n",
    "    pickle.dump(models, output_file)\n",
    "\n",
    "with open('../models/optimized_hyperparameters.pkl', 'wb') as output_file:\n",
    "    pickle.dump(optimized_hyperparameters, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
